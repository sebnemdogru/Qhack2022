{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2522f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "# OpenMP: number of parallel threads.\n",
    "%env OMP_NUM_THREADS=1\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Pennylane\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# Other tools\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ef8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_classes = ['cat', 'dog'] # Subset of CIFAR ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "n_qubits = 8                        # Number of qubits\n",
    "quantum = True                      # If set to \"False\", the dressed quantum circuit is replaced by \n",
    "                                    # An enterily classical net (defined by the next parameter). \n",
    "classical_model = '512_nq_n'           # Possible choices: '512_n','512_nq_n','551_512_n'. [nq=n_qubits, n=num_filtered_classes]\n",
    "step = 0.0007                       # Learning rate\n",
    "batch_size = 8                      # Number of samples for each training step\n",
    "num_epochs = 3                      # Number of training epochs\n",
    "q_depth = 4                         # Depth of the quantum circuit (number of variational layers)\n",
    "gamma_lr_scheduler = 0.1            # Learning rate reduction applied every 3 epochs.                       \n",
    "max_layers = 15                     # Keep 15 even if not all are used.\n",
    "q_delta = 0.01                      # Initial spread of random quantum weights\n",
    "rng_seed = 0                        # Seed for random number generator\n",
    "start_time = time.time()            # Start of the computation timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30af808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires=n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b118c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cb7a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Fixed pre-processing operations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),     # uncomment for data augmentation\n",
    "        #transforms.RandomHorizontalFlip(),     # uncomment for data augmentation\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize input channels using mean values and standard deviations of ImageNet.\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "# =================== begin CIFAR dataset loading ===================\n",
    "trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=data_transforms['train'])\n",
    "testset_full = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=data_transforms['val'])\n",
    "image_datasets_full={'train': trainset_full, 'val': testset_full}\n",
    "\n",
    "# CIFAR classes\n",
    "class_names = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Get indices of samples associated to filtered_classes\n",
    "filtered_labels=[class_names.index(cl) for cl in filtered_classes]\n",
    "sub_indices={'train': [], 'val': []}\n",
    "for phase in ['train', 'val']:\n",
    "    for idx, label in enumerate(image_datasets_full[phase].targets):  \n",
    "        if label in filtered_labels:\n",
    "            sub_indices[phase].append(idx)\n",
    "            \n",
    "# Initialize sub-datasets according to filtered indices\n",
    "image_datasets = {x: torch.utils.data.Subset(image_datasets_full[x], sub_indices[x])\n",
    "                for x in ['train', 'val']}\n",
    "\n",
    "def labels_to_filtered(labels):\n",
    "    \"\"\"Maps CIFAR labels (0,1,2,3,4,5,6,7,8,9) to the index of filtered_labels\"\"\"\n",
    "    return [filtered_labels.index(label) for label in labels]\n",
    "# =================== end CIFAR dataset loading ==========================\n",
    "\n",
    "# Number of samples\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "# Initialize dataloader\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                  batch_size=batch_size, shuffle=True, num_workers=0) for x in ['train', 'val']}\n",
    "\n",
    "# Function to plot images from tensors\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # We apply the inverse of the initial normalization operation.\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd3dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(rng_seed)\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                  batch_size=batch_size, shuffle=True, num_workers=0) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80654b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates. \n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "        \n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis. \n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "    \"\"\"\n",
    "    # In other words it should apply something like :\n",
    "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "    #   CNOT  CNOT  CNOT...  CNOT  \n",
    "    for i in range(0, nqubits - 1, 2): # Loop over even indices: i=0,2,...N-2  \n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1,2): # Loop over odd indices:  i=1,3,...N-3\n",
    "        qml.CNOT(wires=[i, i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6104ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface='torch')\n",
    "def q_net(q_in, q_weights_flat):\n",
    "        \n",
    "        # Reshape weights\n",
    "        q_weights = q_weights_flat.reshape(max_layers, n_qubits)\n",
    "        \n",
    "        # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "        H_layer(n_qubits)\n",
    "        \n",
    "        # Embed features in the quantum node\n",
    "        RY_layer(q_in)\n",
    "       \n",
    "        # Sequence of trainable variational layers\n",
    "        for k in range(q_depth):\n",
    "            entangling_layer(n_qubits)\n",
    "            RY_layer(q_weights[k+1])\n",
    "\n",
    "        # Expectation values in the Z basis\n",
    "        return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038c756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantumnet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.pre_net = nn.Linear(512, n_qubits)\n",
    "            self.q_params = nn.Parameter(q_delta * torch.randn(max_layers * n_qubits))\n",
    "            self.post_net = nn.Linear(n_qubits, len(filtered_classes))\n",
    "\n",
    "        def forward(self, input_features):\n",
    "            pre_out = self.pre_net(input_features) \n",
    "            q_in = torch.tanh(pre_out) * np.pi / 2.0   \n",
    "            \n",
    "            # Apply the quantum circuit to each element of the batch, and append to q_out\n",
    "            q_out = torch.Tensor(0, n_qubits)\n",
    "            q_out = q_out.to(device)\n",
    "            for elem in q_in:\n",
    "                q_out_elem = q_net(elem,self.q_params).float().unsqueeze(0)\n",
    "                q_out = torch.cat((q_out, q_out_elem))\n",
    "            return self.post_net(q_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f279fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dogru\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dogru\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_hybrid = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model_hybrid.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if quantum:\n",
    "    model_hybrid.fc = Quantumnet()\n",
    "\n",
    "elif classical_model == '512_n':\n",
    "    model_hybrid.fc = nn.Linear(512,len(filtered_classes))\n",
    "\n",
    "elif classical_model == '512_nq_n':\n",
    "    model_hybrid.fc = nn.Sequential(nn.Linear(512, n_qubits),torch.nn.ReLU(),nn.Linear(n_qubits, len(filtered_classes))) \n",
    "\n",
    "elif classical_model == '551_512_n':\n",
    "    model_hybrid.fc = nn.Sequential(nn.Linear(512, 512), torch.nn.ReLU(), nn.Linear(512, len(filtered_classes)))\n",
    "\n",
    "# Use CUDA or CPU according to the \"device\" object.\n",
    "model_hybrid = model_hybrid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b5564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8356e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05fe5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_hybrid, step_size=3, gamma=gamma_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f39a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "        since = time.time()\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        best_loss = 10000.0   # Large arbitrary number\n",
    "        best_acc_train = 0.0\n",
    "        best_loss_train = 10000.0  # Large arbitrary number\n",
    "        print('Training started:')\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    # Set model to training mode\n",
    "                    scheduler.step()\n",
    "                    model.train() \n",
    "                else:\n",
    "                    # Set model to evaluate mode\n",
    "                    model.eval()   \n",
    "                \n",
    "                # Iteration loop\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                n_batches = dataset_sizes[phase] // batch_size\n",
    "                it = 0\n",
    "                for inputs, cifar_labels in dataloaders[phase]:\n",
    "                    since_batch = time.time()\n",
    "                    batch_size_ = len(inputs)\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = torch.tensor(labels_to_filtered(cifar_labels))\n",
    "                    labels = labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Track/compute gradient and make an optimization step only when training\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            \n",
    "                    # Print iteration results\n",
    "                    running_loss += loss.item() * batch_size_\n",
    "                    batch_corrects = torch.sum(preds == labels.data).item()\n",
    "                    running_corrects += batch_corrects\n",
    "                    print('Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}'.format(phase, epoch + 1, num_epochs, it + 1, n_batches + 1, time.time() - since_batch), end='\\r', flush=True)\n",
    "                    it += 1\n",
    "                \n",
    "                # Print epoch results\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "                print('Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}             '.format('train' if phase == 'train' else 'val  ', epoch + 1, num_epochs, epoch_loss, epoch_acc))\n",
    "                \n",
    "                # Check if this is the best model wrt previous epochs\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'val' and epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                if phase == 'train' and epoch_acc > best_acc_train:\n",
    "                    best_acc_train = epoch_acc\n",
    "                if phase == 'train' and epoch_loss < best_loss_train:\n",
    "                    best_loss_train = epoch_loss\n",
    "        \n",
    "        # Print final results             \n",
    "        model.load_state_dict(best_model_wts)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best test loss: {:.4f} | Best test accuracy: {:.4f}'.format(best_loss, best_acc))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a3ba114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n",
      "Phase: train Epoch: 1/3 Loss: 0.4296 Acc: 0.8019             \n",
      "Phase: val   Epoch: 1/3 Loss: 0.4042 Acc: 0.8190             \n",
      "Phase: train Epoch: 2/3 Loss: 0.4288 Acc: 0.7992             \n",
      "Phase: val   Epoch: 2/3 Loss: 0.3907 Acc: 0.8240             \n",
      "Phase: train Epoch: 3/3 Loss: 0.4250 Acc: 0.8028             \n",
      "Phase: val   Epoch: 3/3 Loss: 0.3943 Acc: 0.8250             \n",
      "Training completed in 99m 50s\n",
      "Best test loss: 0.3907 | Best test accuracy: 0.8250\n"
     ]
    }
   ],
   "source": [
    "model_hybrid = train_model(model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909f6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa333de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
